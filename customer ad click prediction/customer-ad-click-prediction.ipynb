{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Problem Statement\nWe have a advertising dataset of a marketing agency. Goal is to develop a ML algorithm that predicts if a particular user will click on an advertisement. The dataset has 10 features:\n\n'Daily Time Spent on Site', \n'Age', \n'Area Income',\n'Daily Internet Usage', \n'Ad Topic Line',\n'City',\n'Male',\n'Country',\nTimestamp' \n'Clicked on Ad'.\n\n**'Clicked on Ad'** is the categorical target feature, which has two possible values: 0 (user didn't click) and 1(user clicked). ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-22T19:57:17.994946Z","iopub.execute_input":"2021-11-22T19:57:17.995242Z","iopub.status.idle":"2021-11-22T19:57:18.896353Z","shell.execute_reply.started":"2021-11-22T19:57:17.99521Z","shell.execute_reply":"2021-11-22T19:57:18.895348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/advertising/advertising.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:18.89803Z","iopub.execute_input":"2021-11-22T19:57:18.898278Z","iopub.status.idle":"2021-11-22T19:57:18.938905Z","shell.execute_reply.started":"2021-11-22T19:57:18.898248Z","shell.execute_reply":"2021-11-22T19:57:18.938076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\nprint('features in the dataset:', df.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:18.940064Z","iopub.execute_input":"2021-11-22T19:57:18.940301Z","iopub.status.idle":"2021-11-22T19:57:18.946097Z","shell.execute_reply.started":"2021-11-22T19:57:18.940273Z","shell.execute_reply":"2021-11-22T19:57:18.9451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so we have total 1000 training examples, each with 10 features.","metadata":{}},{"cell_type":"markdown","source":"### Data Analysis","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:18.948794Z","iopub.execute_input":"2021-11-22T19:57:18.949142Z","iopub.status.idle":"2021-11-22T19:57:18.977617Z","shell.execute_reply.started":"2021-11-22T19:57:18.949098Z","shell.execute_reply":"2021-11-22T19:57:18.976738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missingno.matrix(df)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:18.978994Z","iopub.execute_input":"2021-11-22T19:57:18.979772Z","iopub.status.idle":"2021-11-22T19:57:19.690539Z","shell.execute_reply.started":"2021-11-22T19:57:18.979718Z","shell.execute_reply":"2021-11-22T19:57:19.689682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above two cells, we see there is no null values in the dataset, which is a good thing.","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:19.691777Z","iopub.execute_input":"2021-11-22T19:57:19.692024Z","iopub.status.idle":"2021-11-22T19:57:19.699699Z","shell.execute_reply.started":"2021-11-22T19:57:19.691996Z","shell.execute_reply":"2021-11-22T19:57:19.698724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at stats of the non-object features\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:19.701235Z","iopub.execute_input":"2021-11-22T19:57:19.701486Z","iopub.status.idle":"2021-11-22T19:57:19.74051Z","shell.execute_reply.started":"2021-11-22T19:57:19.701456Z","shell.execute_reply":"2021-11-22T19:57:19.739857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's go over each of the non-object features one by one:\n1. **Daily Time Spent on Site:** We see users spend between 32min to 91min on the site with a mean value of 65min, which is quite a large amount of time. This indicates that it is a popular site. We would like to see if there is any corellation with time spend on the site and 'clicked on Ad'.\n\n2. **Age:** The user age ranges from 19years to 61 years with a mean of 36 years, which tells us that the target users are adults.\n\n3. **Area Income:** The minimum users income is around 13k and the maximum user income is 79k, which tells us that the users belongs to different social classes. We would like to further investigate how the income is corelates with the click on the ad.\n\n4. **Daily Internet Usage:** The daily internet use ranges from 104min to 269min. Out of total daily internet use, users spend quite a large amount of time on the site, which ranges from 32 to 91 min. We will check if they both are relates to each other in some way.\n\n5. **Male:** 48% of the users are male. We will check if gender affects the rate of click on the ad. \n\n6. **Clicked on Ad:** From the cell above and the cell below, we see that 50% of the ads were clicked and 50% of the ad weren't clicked by the user. Which tells us that our ad dataset is balanced, which will have a positive affect on training accuracy. ","metadata":{}},{"cell_type":"code","source":"#get the info of the number of ad clicked\nfig = plt.figure(figsize = (5,1))\nsns.countplot(y ='Clicked on Ad', data = df)\nprint(df['Clicked on Ad'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:19.741992Z","iopub.execute_input":"2021-11-22T19:57:19.742314Z","iopub.status.idle":"2021-11-22T19:57:19.924573Z","shell.execute_reply.started":"2021-11-22T19:57:19.742272Z","shell.execute_reply":"2021-11-22T19:57:19.923675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(x='Age',y='Daily Time Spent on Site', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:19.925839Z","iopub.execute_input":"2021-11-22T19:57:19.926091Z","iopub.status.idle":"2021-11-22T19:57:20.615407Z","shell.execute_reply.started":"2021-11-22T19:57:19.926062Z","shell.execute_reply":"2021-11-22T19:57:20.614366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot tells us that the younger users spceially from age 20 to 40, spent most time on the site. So this group of users could be good target group for the ad campaign. We can also say that if a product is targetting a population whose age does not fall into the range 19 to 61, this site is not right platform to advertize the product. ","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x='Age',y='Daily Time Spent on Site', \n                hue='Clicked on Ad', data=df, palette='rocket')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:20.618315Z","iopub.execute_input":"2021-11-22T19:57:20.618601Z","iopub.status.idle":"2021-11-22T19:57:21.069491Z","shell.execute_reply.started":"2021-11-22T19:57:20.618566Z","shell.execute_reply":"2021-11-22T19:57:21.068842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot tells us that all the users who spent less time on the site click on ad. On the other hand, among the 20 to 55 years user group who spent most time on the site apperently don't click on the ad, whereas the same user group who spents less time clicks on ad.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x='Age',y='Daily Internet Usage', \n                hue='Clicked on Ad', data=df, palette='rocket')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:21.070533Z","iopub.execute_input":"2021-11-22T19:57:21.071084Z","iopub.status.idle":"2021-11-22T19:57:21.503061Z","shell.execute_reply.started":"2021-11-22T19:57:21.071052Z","shell.execute_reply":"2021-11-22T19:57:21.502129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see users who are under age of 52 and who spent more time on the internet does not click on the ad, whereas the rest of the users, who usually spent less time on the internet clicks on the ad.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x='Daily Internet Usage',y='Daily Time Spent on Site', \n                hue='Clicked on Ad', data=df, palette='rocket')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:21.504349Z","iopub.execute_input":"2021-11-22T19:57:21.504627Z","iopub.status.idle":"2021-11-22T19:57:21.920697Z","shell.execute_reply.started":"2021-11-22T19:57:21.50459Z","shell.execute_reply":"2021-11-22T19:57:21.919699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again here we see users who usually spend less time on the internet are more likely to click on add and the users who spends more time on the internet and on the site are not. So the target users could be the ones who spend more time on the internet and also on the site.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, hue='Clicked on Ad', vars=['Daily Time Spent on Site', \n                                           'Age', 'Area Income',\n                                           'Daily Internet Usage'],\n            palette='rocket')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:21.922335Z","iopub.execute_input":"2021-11-22T19:57:21.92267Z","iopub.status.idle":"2021-11-22T19:57:27.824304Z","shell.execute_reply.started":"2021-11-22T19:57:21.922627Z","shell.execute_reply":"2021-11-22T19:57:27.823336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pairplot represents the relationshi between the target feature and the explanatory features. \n\nWe also see that users with higher area income who spends more time on the site does not click on ad also relatively younger users with higher income do not click on ads. So this group of users could be the target users. \n\nAgain the users with higher area income who more likely to spend longer time on the site do not click on ad.\n\n","metadata":{}},{"cell_type":"code","source":"plots = ['Daily Time Spent on Site', 'Age', \n         'Area Income','Daily Internet Usage']\nfor i in plots:\n    plt.figure(figsize=(12,6))\n    \n    plt.subplot(2,3,1)\n    sns.boxplot(data=df,x = 'Clicked on Ad', y=i)\n    \n    plt.subplot(2,3,2)\n    sns.boxplot(data=df,y=i)\n    \n    plt.subplot(2,3,3)\n    sns.distplot(df[i],bins=20)\n    plt.tight_layout()\n    plt.title(i)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:27.825577Z","iopub.execute_input":"2021-11-22T19:57:27.826068Z","iopub.status.idle":"2021-11-22T19:57:29.995411Z","shell.execute_reply.started":"2021-11-22T19:57:27.826006Z","shell.execute_reply":"2021-11-22T19:57:29.994814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:29.996509Z","iopub.execute_input":"2021-11-22T19:57:29.996711Z","iopub.status.idle":"2021-11-22T19:57:30.510291Z","shell.execute_reply.started":"2021-11-22T19:57:29.996685Z","shell.execute_reply":"2021-11-22T19:57:30.509342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see Daily Time Spent on Site, Age, Area Income, Daily Intenert Usage are highly correlate with the target variable. Which indicates they are important features and will be useful for ML model.\n\nWe also notice that Daily Time spent on site has strong correlation with other with daily intenet usage, Age, Area Inocme and so does daily internet usage. ","metadata":{}},{"cell_type":"markdown","source":"#### Now let's take a look at the object features: \nAd Topic Line, City, Country, Timestamp ","metadata":{}},{"cell_type":"code","source":"#get the info of the Ad Topic Line\nprint(df['Ad Topic Line'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:30.512429Z","iopub.execute_input":"2021-11-22T19:57:30.51291Z","iopub.status.idle":"2021-11-22T19:57:30.520683Z","shell.execute_reply.started":"2021-11-22T19:57:30.512862Z","shell.execute_reply":"2021-11-22T19:57:30.52006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_features = ['Ad Topic Line', 'City', 'Country', 'Timestamp']\ndf[object_features].describe(include=['O'])  ","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:30.521793Z","iopub.execute_input":"2021-11-22T19:57:30.522047Z","iopub.status.idle":"2021-11-22T19:57:30.552791Z","shell.execute_reply.started":"2021-11-22T19:57:30.522018Z","shell.execute_reply":"2021-11-22T19:57:30.551907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above cell we see that all ad topic lines are unique, which indicates this features has less chace of carying any useful information for the prediction model. There are 969 diffirent cities out of 237 countries. These indicates that the users are not from a spcecific demograhic but from all over the world. Even though we see France repeates 9 times, meaning highest number of visitors are from France but still it just 9 of them. \n\nLet's see if there is any other countries with same number of users.\n","metadata":{}},{"cell_type":"code","source":"pd.crosstab(index=df['Country'], columns='count').sort_values(\n    ['count'], ascending=False).head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:30.554469Z","iopub.execute_input":"2021-11-22T19:57:30.554703Z","iopub.status.idle":"2021-11-22T19:57:30.585914Z","shell.execute_reply.started":"2021-11-22T19:57:30.554675Z","shell.execute_reply":"2021-11-22T19:57:30.585059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there are 237 countries in the dataset and no single country is too dominant. It might be better to remove these features from the dataset. ","metadata":{}},{"cell_type":"code","source":"df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\ndf['Month'] = df['Timestamp'].dt.month\ndf['Day'] = df['Timestamp'].dt.day\ndf['Weekday'] = df['Timestamp'].dt.dayofweek\ndf['Hour'] = df['Timestamp'].dt.hour\ndf = df.drop(['Timestamp'], axis=1)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:30.587301Z","iopub.execute_input":"2021-11-22T19:57:30.588213Z","iopub.status.idle":"2021-11-22T19:57:30.617753Z","shell.execute_reply.started":"2021-11-22T19:57:30.588166Z","shell.execute_reply":"2021-11-22T19:57:30.616886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Month'][df['Clicked on Ad'] == 1].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:30.619458Z","iopub.execute_input":"2021-11-22T19:57:30.619772Z","iopub.status.idle":"2021-11-22T19:57:30.629434Z","shell.execute_reply.started":"2021-11-22T19:57:30.619732Z","shell.execute_reply":"2021-11-22T19:57:30.628604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Day'][df['Clicked on Ad'] == 1].value_counts().sort_index().plot()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:30.631011Z","iopub.execute_input":"2021-11-22T19:57:30.631238Z","iopub.status.idle":"2021-11-22T19:57:30.879607Z","shell.execute_reply.started":"2021-11-22T19:57:30.631211Z","shell.execute_reply":"2021-11-22T19:57:30.87876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Weekday'][df['Clicked on Ad'] == 1].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:30.880809Z","iopub.execute_input":"2021-11-22T19:57:30.881058Z","iopub.status.idle":"2021-11-22T19:57:30.893664Z","shell.execute_reply.started":"2021-11-22T19:57:30.88103Z","shell.execute_reply":"2021-11-22T19:57:30.892669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Hour'][df['Clicked on Ad'] == 1].value_counts().sort_index().plot()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:30.895005Z","iopub.execute_input":"2021-11-22T19:57:30.895231Z","iopub.status.idle":"2021-11-22T19:57:31.115621Z","shell.execute_reply.started":"2021-11-22T19:57:30.895204Z","shell.execute_reply":"2021-11-22T19:57:31.114759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/advertising/advertising.csv')\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\ndf['Month'] = df['Timestamp'].dt.month\ndf['Day'] = df['Timestamp'].dt.day\ndf['Weekday'] = df['Timestamp'].dt.dayofweek\ndf['Hour'] = df['Timestamp'].dt.hour\ndf = df.drop(['Timestamp'], axis=1)\n\ndf.head()\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:31.116857Z","iopub.execute_input":"2021-11-22T19:57:31.117314Z","iopub.status.idle":"2021-11-22T19:57:31.143347Z","shell.execute_reply.started":"2021-11-22T19:57:31.117271Z","shell.execute_reply":"2021-11-22T19:57:31.142504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values:\n","metadata":{}},{"cell_type":"code","source":"df_missing = (df.isnull().sum()/len(df)).sort_values(ascending=False)\ndf_missing.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T20:24:19.1205Z","iopub.execute_input":"2021-11-22T20:24:19.121049Z","iopub.status.idle":"2021-11-22T20:24:19.133187Z","shell.execute_reply.started":"2021-11-22T20:24:19.12099Z","shell.execute_reply":"2021-11-22T20:24:19.132469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ntrain_features = ['Daily Time Spent on Site', 'Age', 'Area Income',\n                   'Daily Internet Usage', 'Male', \n                   'Month', 'Day', 'Weekday', 'Hour']\n\nnumeric_features = ['Daily Time Spent on Site', 'Age', 'Area Income',\n                   'Daily Internet Usage']\n\ncategorical_features = ['Male','Month', 'Day', 'Weekday', 'Hour']\n\nscaler = StandardScaler()\ndf[numeric_features] = scaler.fit_transform(df[numeric_features])\n\nohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\ndf_ohe = ohe.fit_transform(df[categorical_features])\ndf_ohe = pd.DataFrame(df_ohe, columns = [f'ohe_{i}' for i in\n                                        range(df_ohe.shape[1])])\ndf = pd.concat([df,df_ohe], axis=1)\ndf = df.drop(categorical_features, axis =1)\n\ny = df['Clicked on Ad']\nX = df.drop(['Ad Topic Line', 'City', 'Country','Clicked on Ad'], axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state=101)\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n```","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ntrain_features = ['Daily Time Spent on Site', 'Age', 'Area Income',\n                   'Daily Internet Usage', 'Male', \n                   'Month', 'Day', 'Weekday', 'Hour']\n\nnumeric_features = ['Daily Time Spent on Site', 'Age', 'Area Income',\n                   'Daily Internet Usage']\n\n#categorical_features = ['Male','Month', 'Day', 'Weekday', 'Hour']\n\nscaler = StandardScaler()\ndf[numeric_features] = scaler.fit_transform(df[numeric_features])\n\n#ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n#df_ohe = ohe.fit_transform(df[categorical_features])\n#df_ohe = pd.DataFrame(df_ohe, columns = [f'ohe_{i}' for i in range(df_ohe.shape[1])])\n#df = pd.concat([df,df_ohe], axis=1)\n#df = df.drop(categorical_features, axis =1)\n\nX = df[train_features]\ny = df['Clicked on Ad']\n#X = df.drop(['Ad Topic Line', 'City', 'Country','Clicked on Ad'], axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state=101)\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:31.144842Z","iopub.execute_input":"2021-11-22T19:57:31.145563Z","iopub.status.idle":"2021-11-22T19:57:31.592678Z","shell.execute_reply.started":"2021-11-22T19:57:31.145518Z","shell.execute_reply":"2021-11-22T19:57:31.591786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:31.593712Z","iopub.execute_input":"2021-11-22T19:57:31.594441Z","iopub.status.idle":"2021-11-22T19:57:31.609138Z","shell.execute_reply.started":"2021-11-22T19:57:31.594401Z","shell.execute_reply":"2021-11-22T19:57:31.608141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = LogisticRegression(solver='lbfgs')\nmodel1.fit(x_train, y_train)\npredictions_LR = model1.predict(x_test)\n\nprint('\\nLogistic regression accuracy:', accuracy_score(predictions_LR, y_test))\nprint('\\nConfusion Matrix:')\nprint(confusion_matrix(predictions_LR, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:31.610644Z","iopub.execute_input":"2021-11-22T19:57:31.610928Z","iopub.status.idle":"2021-11-22T19:57:31.656191Z","shell.execute_reply.started":"2021-11-22T19:57:31.610878Z","shell.execute_reply":"2021-11-22T19:57:31.655293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, predictions_LR))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:31.66806Z","iopub.execute_input":"2021-11-22T19:57:31.668609Z","iopub.status.idle":"2021-11-22T19:57:31.684955Z","shell.execute_reply.started":"2021-11-22T19:57:31.66857Z","shell.execute_reply":"2021-11-22T19:57:31.683783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix:** The users that are predicted to click on commercials and the actually clicked were 111, the people who were predicted not to click on the commercials and actually did not click on them were 132.\n\nThe people who were predicted to click on commercial and actually did not click on them are 5, and the users who were not predicted to click on the commercials and actually clicked on them are 2.\n\nWe have only a few mislabelled points which is not bad from the given size of the dataset.\n\nClassification Report:\n\nFrom the report obtained, the precision & recall are 0.98 which depicts the predicted values are 98% accurate. Hence the probability that the user can click on the commercial is 0.98 which is a great precision value to get a good model.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel2 = DecisionTreeClassifier()\nmodel2.fit(x_train, y_train)\npredictions_DT = model2.predict(x_test)\n\nprint('\\nDecisionTreeClassifier accuracy:', accuracy_score(predictions_DT, y_test))\nprint('\\nConfusion Matrix:')\nprint(confusion_matrix(predictions_DT, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:31.686522Z","iopub.execute_input":"2021-11-22T19:57:31.687028Z","iopub.status.idle":"2021-11-22T19:57:31.793897Z","shell.execute_reply.started":"2021-11-22T19:57:31.686983Z","shell.execute_reply":"2021-11-22T19:57:31.792992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, predictions_DT))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:31.795478Z","iopub.execute_input":"2021-11-22T19:57:31.79606Z","iopub.status.idle":"2021-11-22T19:57:31.806869Z","shell.execute_reply.started":"2021-11-22T19:57:31.796015Z","shell.execute_reply":"2021-11-22T19:57:31.80589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel3 = XGBClassifier()\nmodel3.fit(x_train, y_train)\npredictions_XGB = model3.predict(x_test)\n\nprint('\\nXGBClassifier accuracy:', accuracy_score(predictions_XGB, y_test))\nprint('\\nConfusion Matrix:')\nprint(confusion_matrix(predictions_XGB, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:31.808344Z","iopub.execute_input":"2021-11-22T19:57:31.808754Z","iopub.status.idle":"2021-11-22T19:57:32.232044Z","shell.execute_reply.started":"2021-11-22T19:57:31.808705Z","shell.execute_reply":"2021-11-22T19:57:32.231245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, predictions_XGB))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:32.235944Z","iopub.execute_input":"2021-11-22T19:57:32.236418Z","iopub.status.idle":"2021-11-22T19:57:32.249059Z","shell.execute_reply.started":"2021-11-22T19:57:32.236376Z","shell.execute_reply":"2021-11-22T19:57:32.248454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importances","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfeature_importances = np.zeros(X.shape[1])\n\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', \n                           boosting_type = 'goss', \n                           n_estimators = 10000, \n                           class_weight = 'balanced')\n\n# Fit the model twice to avoid overfitting\nfor i in range(2):\n    \n    # Split into training and validation set\n    train_features, valid_features, train_y, valid_y = train_test_split(X,\n                                                                        y,\n                                                                        test_size = 0.25, \n                                                                        random_state = i)\n    \n    # Train using early stopping\n    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, \n                                                                               valid_y)], \n              eval_metric = 'auc', verbose = 200)\n    predictions_LGB = model1.predict(valid_features)\n\n    print('\\nLGB accuracy:', accuracy_score(predictions_LGB, valid_y))\n    print('\\nConfusion Matrix:')\n    print(confusion_matrix(predictions_LGB, valid_y))\n    \n    # Record the feature importances\n    feature_importances += model.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2021-11-22T20:53:13.054739Z","iopub.execute_input":"2021-11-22T20:53:13.055065Z","iopub.status.idle":"2021-11-22T20:53:13.204086Z","shell.execute_reply.started":"2021-11-22T20:53:13.05503Z","shell.execute_reply":"2021-11-22T20:53:13.203317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our confusion matrix tells us that the total number of accurate predictions is 158 + 141 = 299. On the other hand, the number of incorrect predictions is 27 + 4 = 31. We can be satisfied with the prediction accuracy of our model.\n\nIt can be concluded that the Decision Tree model showed better performances in comparison to the Logistic Regression model. The confusion matrix shows us that the 308 predictions have been done correctly and that there are only 22 incorrect predictions. Additionally, Decision Tree accuracy is better by about 3% in comparison to the first regression model.","metadata":{}},{"cell_type":"code","source":"# Make sure to average feature importances! \nfeature_importances = feature_importances / 2\nfeature_importances = pd.DataFrame({'feature': list(X.columns),\n                                    'importance': feature_importances}\n                                  ).sort_values('importance', ascending = False)\n\nfeature_importances.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:33.325687Z","iopub.execute_input":"2021-11-22T19:57:33.327886Z","iopub.status.idle":"2021-11-22T19:57:33.343838Z","shell.execute_reply.started":"2021-11-22T19:57:33.32784Z","shell.execute_reply":"2021-11-22T19:57:33.342916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:33.344999Z","iopub.execute_input":"2021-11-22T19:57:33.345223Z","iopub.status.idle":"2021-11-22T19:57:33.366909Z","shell.execute_reply.started":"2021-11-22T19:57:33.345195Z","shell.execute_reply":"2021-11-22T19:57:33.365903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the features with zero importance\nzero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:33.368046Z","iopub.execute_input":"2021-11-22T19:57:33.368346Z","iopub.status.idle":"2021-11-22T19:57:33.390759Z","shell.execute_reply.started":"2021-11-22T19:57:33.36831Z","shell.execute_reply":"2021-11-22T19:57:33.390082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importances(df, threshold = 0.9):\n    \"\"\"\n    Plots 10 most important features and the cumulative importance of features.\n    Prints the number of features needed to reach threshold cumulative importance.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n    threshold : float, default = 0.9\n        Threshold for prining information about cumulative importances\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 18\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    # Cumulative importance plot\n    plt.figure(figsize = (8, 6))\n    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n    plt.title('Cumulative Feature Importance');\n    plt.show();\n    \n    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:33.39197Z","iopub.execute_input":"2021-11-22T19:57:33.392317Z","iopub.status.idle":"2021-11-22T19:57:33.404755Z","shell.execute_reply.started":"2021-11-22T19:57:33.392288Z","shell.execute_reply":"2021-11-22T19:57:33.403754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm_feature_importances = plot_feature_importances(feature_importances,\n                                                   threshold = 0.99)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T19:57:33.406338Z","iopub.execute_input":"2021-11-22T19:57:33.406791Z","iopub.status.idle":"2021-11-22T19:57:33.879162Z","shell.execute_reply.started":"2021-11-22T19:57:33.406753Z","shell.execute_reply":"2021-11-22T19:57:33.87816Z"},"trusted":true},"execution_count":null,"outputs":[]}]}