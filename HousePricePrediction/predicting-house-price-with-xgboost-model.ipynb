{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I will train a model with **gradient boosting**.","metadata":{}},{"cell_type":"code","source":"# Set up code checking\nimport os\nif not os.path.exists(\"../input/train.csv\"):\n    os.symlink(\"../input/home-data-for-ml-course/train.csv\", \"../input/train.csv\")  \n    os.symlink(\"../input/home-data-for-ml-course/test.csv\", \"../input/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex6 import *\nprint(\"Setup Complete\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:54:40.374457Z","iopub.execute_input":"2021-08-02T18:54:40.375123Z","iopub.status.idle":"2021-08-02T18:54:42.762999Z","shell.execute_reply.started":"2021-08-02T18:54:40.375012Z","shell.execute_reply":"2021-08-02T18:54:42.762160Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Setup Complete\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I will work with the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) dataset. \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:54:42.767189Z","iopub.execute_input":"2021-08-02T18:54:42.768655Z","iopub.status.idle":"2021-08-02T18:54:43.025932Z","shell.execute_reply.started":"2021-08-02T18:54:42.768600Z","shell.execute_reply":"2021-08-02T18:54:43.024699Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Build model\n\nIn this step, I'll build and train a model with gradient boosting.\n\n- Begin by setting `my_model_1` to an XGBoost model.  Use the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class, and set the random seed to 0 (`random_state=0`).  **Leave all other parameters as default.**\n- Then, fit the model to the training data in `X_train` and `y_train`.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Define the model\nmy_model_1 = XGBRegressor(random_state = 0)\n\n# Fit the model\nmy_model_1.fit(X_train, y_train)\n\n# Check your answer\nstep_1.a.check()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:54:43.027984Z","iopub.execute_input":"2021-08-02T18:54:43.028301Z","iopub.status.idle":"2021-08-02T18:54:43.841675Z","shell.execute_reply.started":"2021-08-02T18:54:43.028271Z","shell.execute_reply":"2021-08-02T18:54:43.840706Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1.1_Model1A\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#step_1.a.hint()\n#step_1.a.solution()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:54:43.843254Z","iopub.execute_input":"2021-08-02T18:54:43.843541Z","iopub.status.idle":"2021-08-02T18:54:43.849711Z","shell.execute_reply.started":"2021-08-02T18:54:43.843514Z","shell.execute_reply":"2021-08-02T18:54:43.848775Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Set `predictions_1` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Get predictions\npredictions_1 = my_model_1.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:54:43.851071Z","iopub.execute_input":"2021-08-02T18:54:43.851464Z","iopub.status.idle":"2021-08-02T18:54:43.868164Z","shell.execute_reply.started":"2021-08-02T18:54:43.851379Z","shell.execute_reply":"2021-08-02T18:54:43.866960Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Finally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions for the validation set.  Recall that the labels for the validation data are stored in `y_valid`.","metadata":{}},{"cell_type":"code","source":"# Calculate MAE\nmae_1 = mean_absolute_error(predictions_1, y_valid)\n\n# Uncomment to print MAE\n# print(\"Mean Absolute Error:\" , mae_1)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:54:43.870010Z","iopub.execute_input":"2021-08-02T18:54:43.870368Z","iopub.status.idle":"2021-08-02T18:54:43.876924Z","shell.execute_reply.started":"2021-08-02T18:54:43.870336Z","shell.execute_reply":"2021-08-02T18:54:43.876096Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Improve the model\n\nNow that I've trained a default model as baseline, it's time to tinker with the parameters, to see if I can get better performance!\n- Begin by setting `my_model_2` to an XGBoost model, using the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class.  Use what you learned in the previous tutorial to figure out how to change the default parameters (like `n_estimators` and `learning_rate`) to get better results.\n- Then, fit the model to the training data in `X_train` and `y_train`.\n- Set `predictions_2` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`.\n- Finally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions on the validation set.  Recall that the labels for the validation data are stored in `y_valid`.\n\nIn order for this step to be marked correct, I model in `my_model_2` must attain lower MAE than the model in `my_model_1`. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmy_model_2 = XGBRegressor(n_estimators = 1000,max_depth = 3,subsample = 0.8, \n                          colsample_bylevel = 0.7,\n                          learning_rate = 0.03, n_jobs = 4, random_state = 0)\n\n# Fit the model\nmy_model_2.fit(X_train, y_train, early_stopping_rounds = 20, \n              eval_set = [(X_valid, y_valid)],\n              verbose = False\n              )\n\n# Get predictions\npredictions_2 = my_model_2.predict(X_valid)\n\n# Calculate MAE\nmae_2 = mean_absolute_error(predictions_2, y_valid)\nmae_2_cross_valid = -1*cross_val_score(my_model_2, X_train, y_train, scoring = 'neg_mean_absolute_error', \n                        cv = 5)\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_2_cross_valid.mean() )","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:54:43.878230Z","iopub.execute_input":"2021-08-02T18:54:43.878804Z","iopub.status.idle":"2021-08-02T18:54:59.967371Z","shell.execute_reply.started":"2021-08-02T18:54:43.878765Z","shell.execute_reply":"2021-08-02T18:54:59.966313Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Mean Absolute Error: 15315.250347852701\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 3: Break the model\n\nIn this step, I will create a model that performs worse than the original model in Step 1.  This will help to develop an intuition for how to set parameters.  \n- Begin by setting `my_model_3` to an XGBoost model, using the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class.  Use what you learned in the previous tutorial to figure out how to change the default parameters (like `n_estimators` and `learning_rate`) to design a model to get high MAE.\n- Then, fit the model to the training data in `X_train` and `y_train`.\n- Set `predictions_3` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`.\n- Finally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions on the validation set.  Recall that the labels for the validation data are stored in `y_valid`.\n\nIn order for this step to be marked correct, our model in `my_model_3` must attain higher MAE than the model in `my_model_1`. ","metadata":{}},{"cell_type":"code","source":"# Define the model\nmy_model_3 = XGBRegressor(n_estimators = 50, learning_rate = 0.5, n_jobs =4, random_state = 0)\n\n# Fit the model\nmy_model_3.fit(X_train, y_train, early_stopping_rounds = 5,\n              eval_set = [(X_valid, y_valid)], verbose = False)\n\n# Get predictions\npredictions_3 = my_model_3.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(predictions_3, y_valid)\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\" , mae_3)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:54:59.969802Z","iopub.execute_input":"2021-08-02T18:54:59.970492Z","iopub.status.idle":"2021-08-02T18:55:00.151487Z","shell.execute_reply.started":"2021-08-02T18:54:59.970445Z","shell.execute_reply":"2021-08-02T18:55:00.150445Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Mean Absolute Error: 20487.370237585616\n","output_type":"stream"}]},{"cell_type":"code","source":"pred = my_model_2.predict(X_test)\n\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': pred})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:55:28.315370Z","iopub.execute_input":"2021-08-02T18:55:28.315736Z","iopub.status.idle":"2021-08-02T18:55:28.344852Z","shell.execute_reply.started":"2021-08-02T18:55:28.315707Z","shell.execute_reply":"2021-08-02T18:55:28.343980Z"},"trusted":true},"execution_count":9,"outputs":[]}]}