{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:38:45.144675Z","iopub.execute_input":"2021-09-14T17:38:45.144995Z","iopub.status.idle":"2021-09-14T17:38:50.185635Z","shell.execute_reply.started":"2021-09-14T17:38:45.144906Z","shell.execute_reply":"2021-09-14T17:38:50.184919Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2021-09-14 17:38:46.620928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#tf.debugging.set_log_device_placement(True)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:38:50.188867Z","iopub.execute_input":"2021-09-14T17:38:50.189063Z","iopub.status.idle":"2021-09-14T17:38:50.194795Z","shell.execute_reply.started":"2021-09-14T17:38:50.189040Z","shell.execute_reply":"2021-09-14T17:38:50.194110Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"'''gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n  try:\n    tf.config.set_logical_device_configuration(\n        gpus[0],\n        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)'''","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:38:50.197314Z","iopub.execute_input":"2021-09-14T17:38:50.197655Z","iopub.status.idle":"2021-09-14T17:38:50.206588Z","shell.execute_reply.started":"2021-09-14T17:38:50.197621Z","shell.execute_reply":"2021-09-14T17:38:50.205618Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'gpus = tf.config.list_physical_devices(\\'GPU\\')\\nif gpus:\\n  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\\n  try:\\n    tf.config.set_logical_device_configuration(\\n        gpus[0],\\n        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\\n    logical_gpus = tf.config.list_logical_devices(\\'GPU\\')\\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\\n  except RuntimeError as e:\\n    # Virtual devices must be set before GPUs have been initialized\\n    print(e)'"},"metadata":{}}]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\nsubmission_sample = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:38:50.209361Z","iopub.execute_input":"2021-09-14T17:38:50.209711Z","iopub.status.idle":"2021-09-14T17:39:47.710179Z","shell.execute_reply.started":"2021-09-14T17:38:50.209640Z","shell.execute_reply":"2021-09-14T17:39:47.709491Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id       f1        f2         f3        f4       f5        f6       f7  \\\n0   0  0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n1   1  0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n2   2  0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n3   3  0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n4   4  0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n\n         f8            f9  ...     f110    f111     f112      f113      f114  \\\n0  168900.0  3.992400e+14  ... -12.2280  1.7482  1.90960  -7.11570   4378.80   \n1  119810.0  3.874100e+15  ... -56.7580  4.1684  0.34808   4.14200    913.23   \n2  360650.0  1.224500e+13  ...  -5.7688  1.2042  0.26290   8.13120  45119.00   \n3  259490.0  7.781400e+13  ... -34.8580  2.0694  0.79631 -16.33600   4952.40   \n4   65332.0  1.907200e+15  ... -13.6410  1.5298  1.14640  -0.43124   3856.50   \n\n     f115          f116    f117     f118  claim  \n0  1.2096  8.613400e+14   140.1  1.01770      1  \n1  1.2464  7.575100e+15  1861.0  0.28359      0  \n2  1.1764  3.218100e+14  3838.2  0.40690      1  \n3  1.1784  4.533000e+12  4889.1  0.51486      1  \n4  1.4830 -8.991300e+12     NaN  0.23049      1  \n\n[5 rows x 120 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f110</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>claim</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.10859</td>\n      <td>0.004314</td>\n      <td>-37.566</td>\n      <td>0.017364</td>\n      <td>0.28915</td>\n      <td>-10.25100</td>\n      <td>135.12</td>\n      <td>168900.0</td>\n      <td>3.992400e+14</td>\n      <td>...</td>\n      <td>-12.2280</td>\n      <td>1.7482</td>\n      <td>1.90960</td>\n      <td>-7.11570</td>\n      <td>4378.80</td>\n      <td>1.2096</td>\n      <td>8.613400e+14</td>\n      <td>140.1</td>\n      <td>1.01770</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.10090</td>\n      <td>0.299610</td>\n      <td>11822.000</td>\n      <td>0.276500</td>\n      <td>0.45970</td>\n      <td>-0.83733</td>\n      <td>1721.90</td>\n      <td>119810.0</td>\n      <td>3.874100e+15</td>\n      <td>...</td>\n      <td>-56.7580</td>\n      <td>4.1684</td>\n      <td>0.34808</td>\n      <td>4.14200</td>\n      <td>913.23</td>\n      <td>1.2464</td>\n      <td>7.575100e+15</td>\n      <td>1861.0</td>\n      <td>0.28359</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.17803</td>\n      <td>-0.006980</td>\n      <td>907.270</td>\n      <td>0.272140</td>\n      <td>0.45948</td>\n      <td>0.17327</td>\n      <td>2298.00</td>\n      <td>360650.0</td>\n      <td>1.224500e+13</td>\n      <td>...</td>\n      <td>-5.7688</td>\n      <td>1.2042</td>\n      <td>0.26290</td>\n      <td>8.13120</td>\n      <td>45119.00</td>\n      <td>1.1764</td>\n      <td>3.218100e+14</td>\n      <td>3838.2</td>\n      <td>0.40690</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.15236</td>\n      <td>0.007259</td>\n      <td>780.100</td>\n      <td>0.025179</td>\n      <td>0.51947</td>\n      <td>7.49140</td>\n      <td>112.51</td>\n      <td>259490.0</td>\n      <td>7.781400e+13</td>\n      <td>...</td>\n      <td>-34.8580</td>\n      <td>2.0694</td>\n      <td>0.79631</td>\n      <td>-16.33600</td>\n      <td>4952.40</td>\n      <td>1.1784</td>\n      <td>4.533000e+12</td>\n      <td>4889.1</td>\n      <td>0.51486</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.11623</td>\n      <td>0.502900</td>\n      <td>-109.150</td>\n      <td>0.297910</td>\n      <td>0.34490</td>\n      <td>-0.40932</td>\n      <td>2538.90</td>\n      <td>65332.0</td>\n      <td>1.907200e+15</td>\n      <td>...</td>\n      <td>-13.6410</td>\n      <td>1.5298</td>\n      <td>1.14640</td>\n      <td>-0.43124</td>\n      <td>3856.50</td>\n      <td>1.4830</td>\n      <td>-8.991300e+12</td>\n      <td>NaN</td>\n      <td>0.23049</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 120 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['nan_count'] = train.isna().sum(axis=1)\ntest['nan_count'] = test.isna().sum(axis=1)\n\nfeatures = [c for c in train.columns if c not in ('id', 'claim')]\npipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='median', missing_values=np.nan)),\n    ('scaler', QuantileTransformer(n_quantiles=128, output_distribution='uniform')),\n    ('bin', KBinsDiscretizer(n_bins=128, encode='ordinal', strategy='uniform'))\n])\ntrain[features] = pipe.fit_transform(train[features])\ntest[features] = pipe.transform(test[features])\n\nxtrain = train[features]\nytrain = train['claim']\nxtest = test[features]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:39:47.711553Z","iopub.execute_input":"2021-09-14T17:39:47.711802Z","iopub.status.idle":"2021-09-14T17:41:31.360977Z","shell.execute_reply.started":"2021-09-14T17:39:47.711770Z","shell.execute_reply":"2021-09-14T17:41:31.360248Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(xtrain, ytrain, \n                                                    test_size=0.2,\n                                                    random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:41:31.365602Z","iopub.execute_input":"2021-09-14T17:41:31.367608Z","iopub.status.idle":"2021-09-14T17:41:32.404239Z","shell.execute_reply.started":"2021-09-14T17:41:31.367569Z","shell.execute_reply":"2021-09-14T17:41:32.403468Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"'''input_shape = xtrain.shape[1:][0]\n\nmodel = keras.Sequential([\n    layers.Dense(input_shape, activation='relu'),\n    layers.Dense(64, activation='relu'),\n    #layers.Dropout(0.1),\n    layers.Dense(32, activation='relu'),\n    #layers.Dropout(0.2),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1, activation='sigmoid')    \n])\n\nauc = keras.metrics.AUC(name='auc')\noptimizer = keras.optimizers.Adam(lr = 1e-2, epsilon=1e-9, decay=0.001)\nmodel1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = [auc])\n\nhistory = model.fit(x=np.float32(xtrain), \n          y=np.float32(ytrain), \n          batch_size=1024, shuffle=True, \n          epochs=20,\n          validation_data=(X_valid, y_valid),           \n         )'''","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-14T17:41:32.405627Z","iopub.execute_input":"2021-09-14T17:41:32.405883Z","iopub.status.idle":"2021-09-14T17:41:32.414110Z","shell.execute_reply.started":"2021-09-14T17:41:32.405849Z","shell.execute_reply":"2021-09-14T17:41:32.413245Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"\"input_shape = xtrain.shape[1:][0]\\n\\nmodel = keras.Sequential([\\n    layers.Dense(input_shape, activation='relu'),\\n    layers.Dense(64, activation='relu'),\\n    #layers.Dropout(0.1),\\n    layers.Dense(32, activation='relu'),\\n    #layers.Dropout(0.2),\\n    layers.Dense(16, activation='relu'),\\n    layers.Dense(16, activation='relu'),\\n    layers.Dense(1, activation='sigmoid')    \\n])\\n\\nauc = keras.metrics.AUC(name='auc')\\noptimizer = keras.optimizers.Adam(lr = 1e-2, epsilon=1e-9, decay=0.001)\\nmodel1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = [auc])\\n\\nhistory = model.fit(x=np.float32(xtrain), \\n          y=np.float32(ytrain), \\n          batch_size=1024, shuffle=True, \\n          epochs=20,\\n          validation_data=(X_valid, y_valid),           \\n         )\""},"metadata":{}}]},{"cell_type":"code","source":"input_shape = xtrain.shape[1:][0]\n\nmodel1 = keras.Sequential([\n    layers.Dense(input_shape, activation='relu'),\n    layers.Dense(64, activation='relu'),\n    layers.LayerNormalization(axis=-1),\n    #layers.Dropout(0.1),\n    layers.Dense(32, activation='relu'),\n    layers.LayerNormalization(axis=-1),\n    #layers.Dropout(0.2),\n    layers.Dense(64, activation='relu'),\n    layers.LayerNormalization(axis=-1),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(16, activation='relu'),\n    layers.Dense(1, activation='sigmoid')    \n])\n\nauc = keras.metrics.AUC(name='auc')\noptimizer = keras.optimizers.Adam(lr = 2*1e-2, epsilon=1e-9, decay=0.01)\nmodel1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = [auc])\n\nhistory = model1.fit(x=np.float32(xtrain), \n          y=np.float32(ytrain), \n          batch_size=2048, shuffle=True, \n          epochs=200,\n          validation_data=(X_valid, y_valid),           \n         )","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:47:09.488489Z","iopub.execute_input":"2021-09-14T17:47:09.488744Z","iopub.status.idle":"2021-09-14T17:55:58.378388Z","shell.execute_reply.started":"2021-09-14T17:47:09.488716Z","shell.execute_reply":"2021-09-14T17:55:58.377668Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2021-09-14 17:47:10.377872: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 455969444 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n468/468 [==============================] - 4s 7ms/step - loss: 0.6975 - auc: 0.5027 - val_loss: 0.6927 - val_auc: 0.7173\nEpoch 2/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5776 - auc: 0.7481 - val_loss: 0.5136 - val_auc: 0.8002\nEpoch 3/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5133 - auc: 0.8000 - val_loss: 0.5139 - val_auc: 0.8000\nEpoch 4/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5137 - auc: 0.7997 - val_loss: 0.5142 - val_auc: 0.8001\nEpoch 5/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5135 - auc: 0.7996 - val_loss: 0.5132 - val_auc: 0.8012\nEpoch 6/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5128 - auc: 0.8002 - val_loss: 0.5134 - val_auc: 0.8010\nEpoch 7/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5127 - auc: 0.8007 - val_loss: 0.5132 - val_auc: 0.8016\nEpoch 8/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5134 - auc: 0.8010 - val_loss: 0.5128 - val_auc: 0.8016\nEpoch 9/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5124 - auc: 0.8011 - val_loss: 0.5128 - val_auc: 0.8020\nEpoch 10/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5128 - auc: 0.8012 - val_loss: 0.5129 - val_auc: 0.8023\nEpoch 11/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5127 - auc: 0.8015 - val_loss: 0.5129 - val_auc: 0.8018\nEpoch 12/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5124 - auc: 0.8018 - val_loss: 0.5128 - val_auc: 0.8023\nEpoch 13/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5129 - auc: 0.8012 - val_loss: 0.5129 - val_auc: 0.8022\nEpoch 14/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5126 - auc: 0.8018 - val_loss: 0.5132 - val_auc: 0.8015\nEpoch 15/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5128 - auc: 0.8019 - val_loss: 0.5127 - val_auc: 0.8022\nEpoch 16/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5128 - auc: 0.8015 - val_loss: 0.5126 - val_auc: 0.8027\nEpoch 17/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5133 - auc: 0.8019 - val_loss: 0.5125 - val_auc: 0.8030\nEpoch 18/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5123 - auc: 0.8023 - val_loss: 0.5129 - val_auc: 0.8026\nEpoch 19/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5126 - auc: 0.8026 - val_loss: 0.5125 - val_auc: 0.8035\nEpoch 20/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5124 - auc: 0.8027 - val_loss: 0.5125 - val_auc: 0.8037\nEpoch 21/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5118 - auc: 0.8031 - val_loss: 0.5126 - val_auc: 0.8042\nEpoch 22/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5119 - auc: 0.8034 - val_loss: 0.5127 - val_auc: 0.8043\nEpoch 23/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5136 - auc: 0.8028 - val_loss: 0.5127 - val_auc: 0.8049\nEpoch 24/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5129 - auc: 0.8031 - val_loss: 0.5123 - val_auc: 0.8053\nEpoch 25/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5122 - auc: 0.8044 - val_loss: 0.5122 - val_auc: 0.8056\nEpoch 26/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5113 - auc: 0.8054 - val_loss: 0.5123 - val_auc: 0.8055\nEpoch 27/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5114 - auc: 0.8057 - val_loss: 0.5122 - val_auc: 0.8062\nEpoch 28/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5106 - auc: 0.8067 - val_loss: 0.5119 - val_auc: 0.8073\nEpoch 29/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5113 - auc: 0.8063 - val_loss: 0.5118 - val_auc: 0.8072\nEpoch 30/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5118 - auc: 0.8064 - val_loss: 0.5117 - val_auc: 0.8077\nEpoch 31/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5113 - auc: 0.8073 - val_loss: 0.5115 - val_auc: 0.8082\nEpoch 32/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8076 - val_loss: 0.5114 - val_auc: 0.8084\nEpoch 33/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5110 - auc: 0.8079 - val_loss: 0.5114 - val_auc: 0.8085\nEpoch 34/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5114 - auc: 0.8076 - val_loss: 0.5114 - val_auc: 0.8084\nEpoch 35/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5103 - auc: 0.8087 - val_loss: 0.5113 - val_auc: 0.8086\nEpoch 36/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5107 - auc: 0.8084 - val_loss: 0.5116 - val_auc: 0.8084\nEpoch 37/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5113 - auc: 0.8074 - val_loss: 0.5112 - val_auc: 0.8087\nEpoch 38/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5103 - auc: 0.8086 - val_loss: 0.5112 - val_auc: 0.8087\nEpoch 39/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5104 - auc: 0.8089 - val_loss: 0.5112 - val_auc: 0.8088\nEpoch 40/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5115 - auc: 0.8080 - val_loss: 0.5112 - val_auc: 0.8088\nEpoch 41/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8079 - val_loss: 0.5111 - val_auc: 0.8089\nEpoch 42/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5105 - auc: 0.8090 - val_loss: 0.5111 - val_auc: 0.8088\nEpoch 43/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5106 - auc: 0.8089 - val_loss: 0.5111 - val_auc: 0.8089\nEpoch 44/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5123 - auc: 0.8073 - val_loss: 0.5112 - val_auc: 0.8087\nEpoch 45/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5109 - auc: 0.8084 - val_loss: 0.5111 - val_auc: 0.8090\nEpoch 46/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5100 - auc: 0.8091 - val_loss: 0.5110 - val_auc: 0.8090\nEpoch 47/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5119 - auc: 0.8078 - val_loss: 0.5111 - val_auc: 0.8090\nEpoch 48/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5114 - auc: 0.8081 - val_loss: 0.5110 - val_auc: 0.8091\nEpoch 49/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5115 - auc: 0.8081 - val_loss: 0.5112 - val_auc: 0.8091\nEpoch 50/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5111 - auc: 0.8081 - val_loss: 0.5110 - val_auc: 0.8091\nEpoch 51/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5110 - auc: 0.8086 - val_loss: 0.5110 - val_auc: 0.8091\nEpoch 52/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8083 - val_loss: 0.5112 - val_auc: 0.8088\nEpoch 53/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5109 - auc: 0.8085 - val_loss: 0.5110 - val_auc: 0.8091\nEpoch 54/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5109 - auc: 0.8084 - val_loss: 0.5110 - val_auc: 0.8091\nEpoch 55/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5113 - auc: 0.8085 - val_loss: 0.5110 - val_auc: 0.8091\nEpoch 56/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5121 - auc: 0.8075 - val_loss: 0.5110 - val_auc: 0.8092\nEpoch 57/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5101 - auc: 0.8090 - val_loss: 0.5110 - val_auc: 0.8092\nEpoch 58/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5107 - auc: 0.8088 - val_loss: 0.5111 - val_auc: 0.8091\nEpoch 59/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5109 - auc: 0.8084 - val_loss: 0.5109 - val_auc: 0.8092\nEpoch 60/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5108 - auc: 0.8088 - val_loss: 0.5109 - val_auc: 0.8092\nEpoch 61/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5108 - auc: 0.8090 - val_loss: 0.5110 - val_auc: 0.8093\nEpoch 62/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5114 - auc: 0.8084 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 63/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5110 - auc: 0.8086 - val_loss: 0.5110 - val_auc: 0.8092\nEpoch 64/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8084 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 65/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5116 - auc: 0.8079 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 66/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5109 - auc: 0.8087 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 67/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5113 - auc: 0.8082 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 68/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5116 - auc: 0.8085 - val_loss: 0.5110 - val_auc: 0.8093\nEpoch 69/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5099 - auc: 0.8092 - val_loss: 0.5109 - val_auc: 0.8092\nEpoch 70/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5105 - auc: 0.8089 - val_loss: 0.5111 - val_auc: 0.8092\nEpoch 71/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5118 - auc: 0.8080 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 72/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5117 - auc: 0.8079 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 73/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5104 - auc: 0.8089 - val_loss: 0.5110 - val_auc: 0.8091\nEpoch 74/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5106 - auc: 0.8089 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 75/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5105 - auc: 0.8091 - val_loss: 0.5109 - val_auc: 0.8094\nEpoch 76/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5115 - auc: 0.8084 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 77/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5102 - auc: 0.8092 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 78/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5119 - auc: 0.8082 - val_loss: 0.5110 - val_auc: 0.8092\nEpoch 79/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5108 - auc: 0.8088 - val_loss: 0.5108 - val_auc: 0.8093\nEpoch 80/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5108 - auc: 0.8085 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 81/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5114 - auc: 0.8084 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 82/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5111 - auc: 0.8088 - val_loss: 0.5109 - val_auc: 0.8092\nEpoch 83/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5102 - auc: 0.8092 - val_loss: 0.5108 - val_auc: 0.8093\nEpoch 84/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5097 - auc: 0.8094 - val_loss: 0.5110 - val_auc: 0.8094\nEpoch 85/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8085 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 86/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5107 - auc: 0.8089 - val_loss: 0.5108 - val_auc: 0.8093\nEpoch 87/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5115 - auc: 0.8083 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 88/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5104 - auc: 0.8091 - val_loss: 0.5110 - val_auc: 0.8093\nEpoch 89/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8085 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 90/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5103 - auc: 0.8096 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 91/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5113 - auc: 0.8081 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 92/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5109 - auc: 0.8090 - val_loss: 0.5109 - val_auc: 0.8094\nEpoch 93/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5099 - auc: 0.8097 - val_loss: 0.5109 - val_auc: 0.8094\nEpoch 94/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8087 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 95/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5108 - auc: 0.8088 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 96/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5108 - auc: 0.8089 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 97/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5111 - auc: 0.8086 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 98/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5110 - auc: 0.8088 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 99/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8089 - val_loss: 0.5109 - val_auc: 0.8093\nEpoch 100/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5109 - auc: 0.8088 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 101/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5110 - auc: 0.8084 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 102/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5106 - auc: 0.8091 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 103/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5111 - auc: 0.8083 - val_loss: 0.5109 - val_auc: 0.8094\nEpoch 104/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5104 - auc: 0.8095 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 105/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5108 - auc: 0.8088 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 106/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8085 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 107/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8087 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 108/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5107 - auc: 0.8089 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 109/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5109 - auc: 0.8088 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 110/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5110 - auc: 0.8089 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 111/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5105 - auc: 0.8090 - val_loss: 0.5109 - val_auc: 0.8095\nEpoch 112/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5106 - auc: 0.8093 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 113/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5103 - auc: 0.8093 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 114/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5102 - auc: 0.8092 - val_loss: 0.5108 - val_auc: 0.8094\nEpoch 115/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5116 - auc: 0.8085 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 116/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8089 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 117/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5108 - auc: 0.8087 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 118/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5110 - auc: 0.8085 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 119/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5115 - auc: 0.8084 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 120/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5112 - auc: 0.8084 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 121/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5111 - auc: 0.8088 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 122/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5101 - auc: 0.8095 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 123/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5104 - auc: 0.8094 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 124/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5114 - auc: 0.8087 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 125/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5106 - auc: 0.8091 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 126/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5114 - auc: 0.8081 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 127/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8089 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 128/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5097 - auc: 0.8096 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 129/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5102 - auc: 0.8095 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 130/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5106 - auc: 0.8089 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 131/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5116 - auc: 0.8083 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 132/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5102 - auc: 0.8099 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 133/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5108 - auc: 0.8092 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 134/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5112 - auc: 0.8085 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 135/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8083 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 136/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5109 - auc: 0.8090 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 137/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5106 - auc: 0.8092 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 138/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5105 - auc: 0.8091 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 139/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8089 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 140/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5108 - auc: 0.8087 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 141/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5115 - auc: 0.8082 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 142/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5109 - auc: 0.8089 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 143/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5114 - auc: 0.8085 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 144/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8088 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 145/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8091 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 146/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5099 - auc: 0.8096 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 147/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5100 - auc: 0.8094 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 148/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5096 - auc: 0.8099 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 149/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5108 - auc: 0.8089 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 150/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5109 - auc: 0.8090 - val_loss: 0.5108 - val_auc: 0.8095\nEpoch 151/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5101 - auc: 0.8093 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 152/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5111 - auc: 0.8087 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 153/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5100 - auc: 0.8097 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 154/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5099 - auc: 0.8096 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 155/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5113 - auc: 0.8086 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 156/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5106 - auc: 0.8092 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 157/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5114 - auc: 0.8087 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 158/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5108 - auc: 0.8090 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 159/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5101 - auc: 0.8092 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 160/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5105 - auc: 0.8092 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 161/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5112 - auc: 0.8087 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 162/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5115 - auc: 0.8083 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 163/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5102 - auc: 0.8096 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 164/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5115 - auc: 0.8086 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 165/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5108 - auc: 0.8091 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 166/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5106 - auc: 0.8091 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 167/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5110 - auc: 0.8085 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 168/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5098 - auc: 0.8097 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 169/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5114 - auc: 0.8085 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 170/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5109 - auc: 0.8090 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 171/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5107 - auc: 0.8091 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 172/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5096 - auc: 0.8101 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 173/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5114 - auc: 0.8086 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 174/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5110 - auc: 0.8089 - val_loss: 0.5107 - val_auc: 0.8097\nEpoch 175/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5116 - auc: 0.8085 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 176/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5106 - auc: 0.8093 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 177/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5104 - auc: 0.8092 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 178/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5114 - auc: 0.8087 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 179/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5112 - auc: 0.8088 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 180/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5100 - auc: 0.8099 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 181/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5096 - auc: 0.8099 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 182/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5109 - auc: 0.8092 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 183/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5104 - auc: 0.8094 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 184/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5101 - auc: 0.8095 - val_loss: 0.5107 - val_auc: 0.8097\nEpoch 185/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5092 - auc: 0.8103 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 186/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5115 - auc: 0.8084 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 187/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5102 - auc: 0.8095 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 188/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5103 - auc: 0.8095 - val_loss: 0.5107 - val_auc: 0.8097\nEpoch 189/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5102 - auc: 0.8092 - val_loss: 0.5107 - val_auc: 0.8097\nEpoch 190/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5115 - auc: 0.8086 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 191/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5108 - auc: 0.8088 - val_loss: 0.5107 - val_auc: 0.8095\nEpoch 192/200\n468/468 [==============================] - 3s 7ms/step - loss: 0.5107 - auc: 0.8091 - val_loss: 0.5107 - val_auc: 0.8097\nEpoch 193/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5112 - auc: 0.8086 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 194/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5106 - auc: 0.8091 - val_loss: 0.5107 - val_auc: 0.8097\nEpoch 195/200\n468/468 [==============================] - 3s 5ms/step - loss: 0.5108 - auc: 0.8092 - val_loss: 0.5106 - val_auc: 0.8096\nEpoch 196/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5105 - auc: 0.8091 - val_loss: 0.5106 - val_auc: 0.8096\nEpoch 197/200\n468/468 [==============================] - 3s 6ms/step - loss: 0.5100 - auc: 0.8095 - val_loss: 0.5106 - val_auc: 0.8097\nEpoch 198/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5113 - auc: 0.8088 - val_loss: 0.5106 - val_auc: 0.8097\nEpoch 199/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5104 - auc: 0.8097 - val_loss: 0.5107 - val_auc: 0.8096\nEpoch 200/200\n468/468 [==============================] - 2s 5ms/step - loss: 0.5108 - auc: 0.8088 - val_loss: 0.5107 - val_auc: 0.8097\n","output_type":"stream"}]},{"cell_type":"code","source":"submission_sample['claim'] = model1.predict(np.float32(xtest))\nsubmission_sample.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:46:59.628000Z","iopub.status.idle":"2021-09-14T17:46:59.628598Z","shell.execute_reply.started":"2021-09-14T17:46:59.628361Z","shell.execute_reply":"2021-09-14T17:46:59.628384Z"},"trusted":true},"execution_count":null,"outputs":[]}]}