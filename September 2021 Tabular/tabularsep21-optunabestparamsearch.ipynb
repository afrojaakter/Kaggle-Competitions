{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBClassifier\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import ensemble\nfrom sklearn.metrics import roc_auc_score\nimport optuna\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-14T17:56:48.190650Z","iopub.execute_input":"2021-09-14T17:56:48.191194Z","iopub.status.idle":"2021-09-14T17:56:49.713889Z","shell.execute_reply.started":"2021-09-14T17:56:48.191098Z","shell.execute_reply":"2021-09-14T17:56:49.712970Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabularsep21-kfolddataset/train_10folds.csv')\ntest = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\nsubmission_data = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:56:49.717218Z","iopub.execute_input":"2021-09-14T17:56:49.717816Z","iopub.status.idle":"2021-09-14T17:57:33.281344Z","shell.execute_reply.started":"2021-09-14T17:56:49.717786Z","shell.execute_reply":"2021-09-14T17:57:33.280590Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"        f1        f2         f3        f4       f5        f6       f7  \\\n0  0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n1  0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n2  0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n3  0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n4  0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n\n         f8            f9        f10  ...     f112      f113      f114  \\\n0  168900.0  3.992400e+14     86.489  ...  1.90960  -7.11570   4378.80   \n1  119810.0  3.874100e+15   9953.600  ...  0.34808   4.14200    913.23   \n2  360650.0  1.224500e+13  15827.000  ...  0.26290   8.13120  45119.00   \n3  259490.0  7.781400e+13    -36.837  ...  0.79631 -16.33600   4952.40   \n4   65332.0  1.907200e+15    144.120  ...  1.14640  -0.43124   3856.50   \n\n     f115          f116     f117     f118  claim  nan_count  kfold  \n0  1.2096  8.613400e+14    140.1  1.01770      1          1      3  \n1  1.2464  7.575100e+15   1861.0  0.28359      0          0      8  \n2  1.1764  3.218100e+14   3838.2  0.40690      1          5      6  \n3  1.1784  4.533000e+12   4889.1  0.51486      1          2      6  \n4  1.4830 -8.991300e+12  10002.0  0.23049      1          8      9  \n\n[5 rows x 121 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>f10</th>\n      <th>...</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>claim</th>\n      <th>nan_count</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.10859</td>\n      <td>0.004314</td>\n      <td>-37.566</td>\n      <td>0.017364</td>\n      <td>0.28915</td>\n      <td>-10.25100</td>\n      <td>135.12</td>\n      <td>168900.0</td>\n      <td>3.992400e+14</td>\n      <td>86.489</td>\n      <td>...</td>\n      <td>1.90960</td>\n      <td>-7.11570</td>\n      <td>4378.80</td>\n      <td>1.2096</td>\n      <td>8.613400e+14</td>\n      <td>140.1</td>\n      <td>1.01770</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.10090</td>\n      <td>0.299610</td>\n      <td>11822.000</td>\n      <td>0.276500</td>\n      <td>0.45970</td>\n      <td>-0.83733</td>\n      <td>1721.90</td>\n      <td>119810.0</td>\n      <td>3.874100e+15</td>\n      <td>9953.600</td>\n      <td>...</td>\n      <td>0.34808</td>\n      <td>4.14200</td>\n      <td>913.23</td>\n      <td>1.2464</td>\n      <td>7.575100e+15</td>\n      <td>1861.0</td>\n      <td>0.28359</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.17803</td>\n      <td>-0.006980</td>\n      <td>907.270</td>\n      <td>0.272140</td>\n      <td>0.45948</td>\n      <td>0.17327</td>\n      <td>2298.00</td>\n      <td>360650.0</td>\n      <td>1.224500e+13</td>\n      <td>15827.000</td>\n      <td>...</td>\n      <td>0.26290</td>\n      <td>8.13120</td>\n      <td>45119.00</td>\n      <td>1.1764</td>\n      <td>3.218100e+14</td>\n      <td>3838.2</td>\n      <td>0.40690</td>\n      <td>1</td>\n      <td>5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.15236</td>\n      <td>0.007259</td>\n      <td>780.100</td>\n      <td>0.025179</td>\n      <td>0.51947</td>\n      <td>7.49140</td>\n      <td>112.51</td>\n      <td>259490.0</td>\n      <td>7.781400e+13</td>\n      <td>-36.837</td>\n      <td>...</td>\n      <td>0.79631</td>\n      <td>-16.33600</td>\n      <td>4952.40</td>\n      <td>1.1784</td>\n      <td>4.533000e+12</td>\n      <td>4889.1</td>\n      <td>0.51486</td>\n      <td>1</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.11623</td>\n      <td>0.502900</td>\n      <td>-109.150</td>\n      <td>0.297910</td>\n      <td>0.34490</td>\n      <td>-0.40932</td>\n      <td>2538.90</td>\n      <td>65332.0</td>\n      <td>1.907200e+15</td>\n      <td>144.120</td>\n      <td>...</td>\n      <td>1.14640</td>\n      <td>-0.43124</td>\n      <td>3856.50</td>\n      <td>1.4830</td>\n      <td>-8.991300e+12</td>\n      <td>10002.0</td>\n      <td>0.23049</td>\n      <td>1</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 121 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"features = [col for col in df_train.columns if col not in ('claim', 'kfold')]\nfeatures","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:57:33.282600Z","iopub.execute_input":"2021-09-14T17:57:33.283337Z","iopub.status.idle":"2021-09-14T17:57:33.291829Z","shell.execute_reply.started":"2021-09-14T17:57:33.283300Z","shell.execute_reply":"2021-09-14T17:57:33.291156Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['f1',\n 'f2',\n 'f3',\n 'f4',\n 'f5',\n 'f6',\n 'f7',\n 'f8',\n 'f9',\n 'f10',\n 'f11',\n 'f12',\n 'f13',\n 'f14',\n 'f15',\n 'f16',\n 'f17',\n 'f18',\n 'f19',\n 'f20',\n 'f21',\n 'f22',\n 'f23',\n 'f24',\n 'f25',\n 'f26',\n 'f27',\n 'f28',\n 'f29',\n 'f30',\n 'f31',\n 'f32',\n 'f33',\n 'f34',\n 'f35',\n 'f36',\n 'f37',\n 'f38',\n 'f39',\n 'f40',\n 'f41',\n 'f42',\n 'f43',\n 'f44',\n 'f45',\n 'f46',\n 'f47',\n 'f48',\n 'f49',\n 'f50',\n 'f51',\n 'f52',\n 'f53',\n 'f54',\n 'f55',\n 'f56',\n 'f57',\n 'f58',\n 'f59',\n 'f60',\n 'f61',\n 'f62',\n 'f63',\n 'f64',\n 'f65',\n 'f66',\n 'f67',\n 'f68',\n 'f69',\n 'f70',\n 'f71',\n 'f72',\n 'f73',\n 'f74',\n 'f75',\n 'f76',\n 'f77',\n 'f78',\n 'f79',\n 'f80',\n 'f81',\n 'f82',\n 'f83',\n 'f84',\n 'f85',\n 'f86',\n 'f87',\n 'f88',\n 'f89',\n 'f90',\n 'f91',\n 'f92',\n 'f93',\n 'f94',\n 'f95',\n 'f96',\n 'f97',\n 'f98',\n 'f99',\n 'f100',\n 'f101',\n 'f102',\n 'f103',\n 'f104',\n 'f105',\n 'f106',\n 'f107',\n 'f108',\n 'f109',\n 'f110',\n 'f111',\n 'f112',\n 'f113',\n 'f114',\n 'f115',\n 'f116',\n 'f117',\n 'f118',\n 'nan_count']"},"metadata":{}}]},{"cell_type":"code","source":"test['nan_count'] = test.isnull().sum(axis=1)\nxtest = test[features]\nmode = df_train[features].mode().iloc[0]\nxtest = xtest.fillna(mode)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:57:33.293755Z","iopub.execute_input":"2021-09-14T17:57:33.294027Z","iopub.status.idle":"2021-09-14T17:57:37.160230Z","shell.execute_reply.started":"2021-09-14T17:57:33.293995Z","shell.execute_reply":"2021-09-14T17:57:37.159450Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"xtrain = df_train[features]\nscaler = preprocessing.StandardScaler()\n\ntest[features] = scaler.fit_transform(xtest[features])\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:57:37.161657Z","iopub.execute_input":"2021-09-14T17:57:37.161926Z","iopub.status.idle":"2021-09-14T17:57:53.294112Z","shell.execute_reply.started":"2021-09-14T17:57:37.161893Z","shell.execute_reply":"2021-09-14T17:57:53.293457Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       id        f1        f2        f3        f4        f5        f6  \\\n0  957919  1.747278  0.948442 -0.427147 -0.840645  0.095294  0.468778   \n1  957920  0.909508  0.170491 -0.353351  2.503614  0.188642 -0.556304   \n2  957921  0.690577  0.661839 -0.515251 -0.731419 -0.063751  0.191682   \n3  957922 -0.841062  0.324478 -0.474272  3.122008  0.426413 -0.039023   \n4  957923 -0.240760 -2.435226  1.035997  0.701076  1.003687 -0.830404   \n\n         f7        f8        f9  ...      f110      f111      f112      f113  \\\n0 -0.832226 -1.072016 -0.779770  ... -0.131863 -0.001763 -0.533905 -1.254592   \n1 -0.593130  0.528871 -0.524140  ...  0.986689 -0.553983 -0.566362  0.986925   \n2  0.818023 -0.160850 -0.609422  ... -0.708205  0.077831 -0.584945 -0.513748   \n3 -0.979934 -0.136020 -0.795736  ...  0.944134 -0.614268 -0.543467 -0.050570   \n4  1.108725  0.161900 -0.780536  ...  0.061811  1.876614 -0.526856 -0.678597   \n\n       f114      f115      f116      f117      f118  nan_count  \n0  0.194415 -0.124117 -0.629749 -0.634560 -0.250647  -0.442610  \n1  0.287187 -0.502827  0.022404  0.798596 -0.695993  -0.935935  \n2 -0.283915 -0.466968 -0.630140  0.552108 -1.045977  -0.442610  \n3 -0.402660 -0.517696  2.560428  0.654220 -0.579111  -0.935935  \n4 -0.146556  0.355174  2.579090 -0.562310 -0.557469  -0.935935  \n\n[5 rows x 120 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>...</th>\n      <th>f110</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>nan_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>957919</td>\n      <td>1.747278</td>\n      <td>0.948442</td>\n      <td>-0.427147</td>\n      <td>-0.840645</td>\n      <td>0.095294</td>\n      <td>0.468778</td>\n      <td>-0.832226</td>\n      <td>-1.072016</td>\n      <td>-0.779770</td>\n      <td>...</td>\n      <td>-0.131863</td>\n      <td>-0.001763</td>\n      <td>-0.533905</td>\n      <td>-1.254592</td>\n      <td>0.194415</td>\n      <td>-0.124117</td>\n      <td>-0.629749</td>\n      <td>-0.634560</td>\n      <td>-0.250647</td>\n      <td>-0.442610</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>957920</td>\n      <td>0.909508</td>\n      <td>0.170491</td>\n      <td>-0.353351</td>\n      <td>2.503614</td>\n      <td>0.188642</td>\n      <td>-0.556304</td>\n      <td>-0.593130</td>\n      <td>0.528871</td>\n      <td>-0.524140</td>\n      <td>...</td>\n      <td>0.986689</td>\n      <td>-0.553983</td>\n      <td>-0.566362</td>\n      <td>0.986925</td>\n      <td>0.287187</td>\n      <td>-0.502827</td>\n      <td>0.022404</td>\n      <td>0.798596</td>\n      <td>-0.695993</td>\n      <td>-0.935935</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>957921</td>\n      <td>0.690577</td>\n      <td>0.661839</td>\n      <td>-0.515251</td>\n      <td>-0.731419</td>\n      <td>-0.063751</td>\n      <td>0.191682</td>\n      <td>0.818023</td>\n      <td>-0.160850</td>\n      <td>-0.609422</td>\n      <td>...</td>\n      <td>-0.708205</td>\n      <td>0.077831</td>\n      <td>-0.584945</td>\n      <td>-0.513748</td>\n      <td>-0.283915</td>\n      <td>-0.466968</td>\n      <td>-0.630140</td>\n      <td>0.552108</td>\n      <td>-1.045977</td>\n      <td>-0.442610</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>957922</td>\n      <td>-0.841062</td>\n      <td>0.324478</td>\n      <td>-0.474272</td>\n      <td>3.122008</td>\n      <td>0.426413</td>\n      <td>-0.039023</td>\n      <td>-0.979934</td>\n      <td>-0.136020</td>\n      <td>-0.795736</td>\n      <td>...</td>\n      <td>0.944134</td>\n      <td>-0.614268</td>\n      <td>-0.543467</td>\n      <td>-0.050570</td>\n      <td>-0.402660</td>\n      <td>-0.517696</td>\n      <td>2.560428</td>\n      <td>0.654220</td>\n      <td>-0.579111</td>\n      <td>-0.935935</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>957923</td>\n      <td>-0.240760</td>\n      <td>-2.435226</td>\n      <td>1.035997</td>\n      <td>0.701076</td>\n      <td>1.003687</td>\n      <td>-0.830404</td>\n      <td>1.108725</td>\n      <td>0.161900</td>\n      <td>-0.780536</td>\n      <td>...</td>\n      <td>0.061811</td>\n      <td>1.876614</td>\n      <td>-0.526856</td>\n      <td>-0.678597</td>\n      <td>-0.146556</td>\n      <td>0.355174</td>\n      <td>2.579090</td>\n      <td>-0.562310</td>\n      <td>-0.557469</td>\n      <td>-0.935935</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 120 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train[features].head()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T17:57:53.295180Z","iopub.execute_input":"2021-09-14T17:57:53.295504Z","iopub.status.idle":"2021-09-14T17:57:53.587043Z","shell.execute_reply.started":"2021-09-14T17:57:53.295468Z","shell.execute_reply":"2021-09-14T17:57:53.586156Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        f1        f2         f3        f4       f5        f6       f7  \\\n0  0.10859  0.004314    -37.566  0.017364  0.28915 -10.25100   135.12   \n1  0.10090  0.299610  11822.000  0.276500  0.45970  -0.83733  1721.90   \n2  0.17803 -0.006980    907.270  0.272140  0.45948   0.17327  2298.00   \n3  0.15236  0.007259    780.100  0.025179  0.51947   7.49140   112.51   \n4  0.11623  0.502900   -109.150  0.297910  0.34490  -0.40932  2538.90   \n\n         f8            f9        f10  ...     f110    f111     f112      f113  \\\n0  168900.0  3.992400e+14     86.489  ... -12.2280  1.7482  1.90960  -7.11570   \n1  119810.0  3.874100e+15   9953.600  ... -56.7580  4.1684  0.34808   4.14200   \n2  360650.0  1.224500e+13  15827.000  ...  -5.7688  1.2042  0.26290   8.13120   \n3  259490.0  7.781400e+13    -36.837  ... -34.8580  2.0694  0.79631 -16.33600   \n4   65332.0  1.907200e+15    144.120  ... -13.6410  1.5298  1.14640  -0.43124   \n\n       f114    f115          f116     f117     f118  nan_count  \n0   4378.80  1.2096  8.613400e+14    140.1  1.01770          1  \n1    913.23  1.2464  7.575100e+15   1861.0  0.28359          0  \n2  45119.00  1.1764  3.218100e+14   3838.2  0.40690          5  \n3   4952.40  1.1784  4.533000e+12   4889.1  0.51486          2  \n4   3856.50  1.4830 -8.991300e+12  10002.0  0.23049          8  \n\n[5 rows x 119 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n      <th>f4</th>\n      <th>f5</th>\n      <th>f6</th>\n      <th>f7</th>\n      <th>f8</th>\n      <th>f9</th>\n      <th>f10</th>\n      <th>...</th>\n      <th>f110</th>\n      <th>f111</th>\n      <th>f112</th>\n      <th>f113</th>\n      <th>f114</th>\n      <th>f115</th>\n      <th>f116</th>\n      <th>f117</th>\n      <th>f118</th>\n      <th>nan_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.10859</td>\n      <td>0.004314</td>\n      <td>-37.566</td>\n      <td>0.017364</td>\n      <td>0.28915</td>\n      <td>-10.25100</td>\n      <td>135.12</td>\n      <td>168900.0</td>\n      <td>3.992400e+14</td>\n      <td>86.489</td>\n      <td>...</td>\n      <td>-12.2280</td>\n      <td>1.7482</td>\n      <td>1.90960</td>\n      <td>-7.11570</td>\n      <td>4378.80</td>\n      <td>1.2096</td>\n      <td>8.613400e+14</td>\n      <td>140.1</td>\n      <td>1.01770</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.10090</td>\n      <td>0.299610</td>\n      <td>11822.000</td>\n      <td>0.276500</td>\n      <td>0.45970</td>\n      <td>-0.83733</td>\n      <td>1721.90</td>\n      <td>119810.0</td>\n      <td>3.874100e+15</td>\n      <td>9953.600</td>\n      <td>...</td>\n      <td>-56.7580</td>\n      <td>4.1684</td>\n      <td>0.34808</td>\n      <td>4.14200</td>\n      <td>913.23</td>\n      <td>1.2464</td>\n      <td>7.575100e+15</td>\n      <td>1861.0</td>\n      <td>0.28359</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.17803</td>\n      <td>-0.006980</td>\n      <td>907.270</td>\n      <td>0.272140</td>\n      <td>0.45948</td>\n      <td>0.17327</td>\n      <td>2298.00</td>\n      <td>360650.0</td>\n      <td>1.224500e+13</td>\n      <td>15827.000</td>\n      <td>...</td>\n      <td>-5.7688</td>\n      <td>1.2042</td>\n      <td>0.26290</td>\n      <td>8.13120</td>\n      <td>45119.00</td>\n      <td>1.1764</td>\n      <td>3.218100e+14</td>\n      <td>3838.2</td>\n      <td>0.40690</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.15236</td>\n      <td>0.007259</td>\n      <td>780.100</td>\n      <td>0.025179</td>\n      <td>0.51947</td>\n      <td>7.49140</td>\n      <td>112.51</td>\n      <td>259490.0</td>\n      <td>7.781400e+13</td>\n      <td>-36.837</td>\n      <td>...</td>\n      <td>-34.8580</td>\n      <td>2.0694</td>\n      <td>0.79631</td>\n      <td>-16.33600</td>\n      <td>4952.40</td>\n      <td>1.1784</td>\n      <td>4.533000e+12</td>\n      <td>4889.1</td>\n      <td>0.51486</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.11623</td>\n      <td>0.502900</td>\n      <td>-109.150</td>\n      <td>0.297910</td>\n      <td>0.34490</td>\n      <td>-0.40932</td>\n      <td>2538.90</td>\n      <td>65332.0</td>\n      <td>1.907200e+15</td>\n      <td>144.120</td>\n      <td>...</td>\n      <td>-13.6410</td>\n      <td>1.5298</td>\n      <td>1.14640</td>\n      <td>-0.43124</td>\n      <td>3856.50</td>\n      <td>1.4830</td>\n      <td>-8.991300e+12</td>\n      <td>10002.0</td>\n      <td>0.23049</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 119 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"xtrain.shape, df_train.claim.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T18:03:11.554987Z","iopub.execute_input":"2021-09-14T18:03:11.555752Z","iopub.status.idle":"2021-09-14T18:03:11.561306Z","shell.execute_reply.started":"2021-09-14T18:03:11.555709Z","shell.execute_reply":"2021-09-14T18:03:11.560651Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((957919, 119), (957919,))"},"metadata":{}}]},{"cell_type":"code","source":"def run(trial):\n    #optimize in one fold\n    fold = 0\n    xtrain = df_train[df_train.kfold != fold].reset_index(drop=True)\n    xvalid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n    \n    xtrain = scaler.transform(xtrain[features])\n    xvalid = scaler.transform(xvalid[features])\n    \n    learning_rate = trial.suggest_float('learning_rate', 1e-2, 0.8, log=True)  \n    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 0.9)\n    max_depth = trial.suggest_int('max_depth', 1, 9)\n    reg_lambda = trial.suggest_float('reg_lembda', 1e-5, 100.0)\n    reg_alpha = trial.suggest_float('reg_alpha', 1e-5, 100.0)\n    subsample = trial.suggest_float('sumsample', 0.1, 0.9)\n    alpha = trial.suggest_int('alpha', 0, 100)\n    #n_estimator = trial.suggest_int('n_estimator', 500, 4000)\n    \n    model = XGBClassifier(random_state = 0,\n                          #n_estimator = n_estimator,\n                          learning_rate = learning_rate,\n                          colsample_bytree = colsample_bytree,\n                          max_depth = max_depth,\n                          reg_lambda = reg_lambda,\n                          reg_alpha = reg_alpha,\n                          subsample = subsample,\n                          alpha = alpha, \n                          tree_method = 'gpu_hist',\n                          gpu_id = 0,\n                          predictor = 'gpu_predictor',\n                          #boosting='gbdt',\n                          #metric='multiclass',\n                          eval_metric='mlogloss'\n                         )\n    model.fit(xtrain, ytrain)\n    valid_preds = model.predict(xvalid)\n    \n    roc_auc = roc_auc_score(yvalid, valid_preds)\n    return roc_auc\n\nstudy = optuna.create_study(direction='maximize')   \nstudy.optimize(run, n_trials=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T18:34:21.331825Z","iopub.execute_input":"2021-09-14T18:34:21.332393Z","iopub.status.idle":"2021-09-14T18:34:56.342817Z","shell.execute_reply.started":"2021-09-14T18:34:21.332356Z","shell.execute_reply":"2021-09-14T18:34:56.342155Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2021-09-14 18:34:21,340]\u001b[0m A new study created in memory with name: no-name-51775d13-1a48-40fa-9dbd-41afc0622da2\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\u001b[32m[I 2021-09-14 18:34:28,086]\u001b[0m Trial 0 finished with value: 0.7730736313698521 and parameters: {'learning_rate': 0.08674869015674984, 'colsample_bytree': 0.894194483453287, 'max_depth': 7, 'reg_lembda': 96.69872679770177, 'reg_alpha': 89.60570782924681, 'sumsample': 0.2833840188066744, 'alpha': 35}. Best is trial 0 with value: 0.7730736313698521.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\u001b[32m[I 2021-09-14 18:34:36,935]\u001b[0m Trial 1 finished with value: 0.7725653097363466 and parameters: {'learning_rate': 0.10276011680592632, 'colsample_bytree': 0.752270480898139, 'max_depth': 9, 'reg_lembda': 92.34692521110107, 'reg_alpha': 34.23477034695513, 'sumsample': 0.30034294429550706, 'alpha': 24}. Best is trial 0 with value: 0.7730736313698521.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\u001b[32m[I 2021-09-14 18:34:43,749]\u001b[0m Trial 2 finished with value: 0.7699159648269965 and parameters: {'learning_rate': 0.48146552270090415, 'colsample_bytree': 0.6008647230405472, 'max_depth': 6, 'reg_lembda': 55.12208037466225, 'reg_alpha': 8.716609503818928, 'sumsample': 0.41297149258482724, 'alpha': 45}. Best is trial 0 with value: 0.7730736313698521.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\u001b[32m[I 2021-09-14 18:34:49,490]\u001b[0m Trial 3 finished with value: 0.7727744859587283 and parameters: {'learning_rate': 0.13441448704808936, 'colsample_bytree': 0.5352116112839042, 'max_depth': 3, 'reg_lembda': 48.46679537956, 'reg_alpha': 93.41329827596799, 'sumsample': 0.7484402745109086, 'alpha': 49}. Best is trial 0 with value: 0.7730736313698521.\u001b[0m\n/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\u001b[32m[I 2021-09-14 18:34:56,338]\u001b[0m Trial 4 finished with value: 0.7727233857716281 and parameters: {'learning_rate': 0.02477230789120222, 'colsample_bytree': 0.849680159350675, 'max_depth': 8, 'reg_lembda': 2.0433900279099033, 'reg_alpha': 85.33612194649584, 'sumsample': 0.7937615692965262, 'alpha': 1}. Best is trial 0 with value: 0.7730736313698521.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"best_params = study.best_params\nbest_params","metadata":{"execution":{"iopub.status.busy":"2021-09-14T18:34:56.344385Z","iopub.execute_input":"2021-09-14T18:34:56.344634Z","iopub.status.idle":"2021-09-14T18:34:56.352496Z","shell.execute_reply.started":"2021-09-14T18:34:56.344601Z","shell.execute_reply":"2021-09-14T18:34:56.351504Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'learning_rate': 0.08674869015674984,\n 'colsample_bytree': 0.894194483453287,\n 'max_depth': 7,\n 'reg_lembda': 96.69872679770177,\n 'reg_alpha': 89.60570782924681,\n 'sumsample': 0.2833840188066744,\n 'alpha': 35}"},"metadata":{}}]},{"cell_type":"code","source":"final_preds = []\nfor fold in range(10):\n    xtrain = df_train[df_train.kfold != fold].reset_index(drop=True)\n    xvalid = df_train[df_train.kfold == fold].reset_index(drop=True)\n    \n    ytrain = xtrain.claim\n    yvalid = xvalid.claim\n    \n    xtrain = scaler.transform(xtrain[features])\n    xvalid = scaler.transform(xvalid[features])\n    \n    best_params = {'learning_rate': 0.08674869015674984,\n                     'colsample_bytree': 0.894194483453287,\n                     'max_depth': 7,\n                     'reg_lembda': 96.69872679770177,\n                     'reg_alpha': 89.60570782924681,\n                     'sumsample': 0.2833840188066744,\n                     'alpha': 35}\n    \n    model = XGBClassifier(random_state = 0,\n                          #n_estimator = 500,\n                          **best_params, \n                          tree_method = 'gpu_hist',\n                          gpu_id = 0,\n                          predictor = 'gpu_predictor',\n                          eval_metric='mlogloss'\n                         )\n    model.fit(xtrain, ytrain)\n    valid_preds = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_preds.append(test_preds)\n    print(fold, roc_auc_score(yvalid, valid_preds))\n\npreds = np.mean(np.column_stack(final_preds), axis=1)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T18:35:37.998686Z","iopub.execute_input":"2021-09-14T18:35:37.998957Z","iopub.status.idle":"2021-09-14T18:37:03.042846Z","shell.execute_reply.started":"2021-09-14T18:35:37.998928Z","shell.execute_reply":"2021-09-14T18:37:03.041278Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:35:41] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n0 0.7729688808794686\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:35:50] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n1 0.7721575348665891\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:35:58] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n2 0.7726490523973744\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:36:07] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n3 0.771271484627671\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:36:16] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n4 0.7742007667775599\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:36:24] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n5 0.771435915721172\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:36:33] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n6 0.7753392224440004\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:36:41] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n7 0.7728919237247462\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:36:50] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n8 0.773525432758865\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[18:36:58] WARNING: ../src/learner.cc:573: \nParameters: { \"reg_lembda\", \"sumsample\" } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n9 0.7733823517762851\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = np.mean(np.column_stack(final_preds), axis=1)\npreds","metadata":{"execution":{"iopub.status.busy":"2021-09-14T18:37:14.452607Z","iopub.execute_input":"2021-09-14T18:37:14.452937Z","iopub.status.idle":"2021-09-14T18:37:14.519376Z","shell.execute_reply.started":"2021-09-14T18:37:14.452899Z","shell.execute_reply":"2021-09-14T18:37:14.518645Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"array([1., 1., 1., ..., 1., 1., 1.])"},"metadata":{}}]},{"cell_type":"code","source":"submission_data.claim = preds\nsubmission_data.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T18:37:17.766503Z","iopub.execute_input":"2021-09-14T18:37:17.767212Z","iopub.status.idle":"2021-09-14T18:37:18.781823Z","shell.execute_reply.started":"2021-09-14T18:37:17.767178Z","shell.execute_reply":"2021-09-14T18:37:18.781044Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"submission_data.claim.value_counts().to_frame().T","metadata":{"execution":{"iopub.status.busy":"2021-09-14T18:37:20.789263Z","iopub.execute_input":"2021-09-14T18:37:20.789970Z","iopub.status.idle":"2021-09-14T18:37:20.812109Z","shell.execute_reply.started":"2021-09-14T18:37:20.789932Z","shell.execute_reply":"2021-09-14T18:37:20.811445Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"          1.0   0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0.0\nclaim  491010  1380  453  257  139   93   58   36   29   13    6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1.0</th>\n      <th>0.9</th>\n      <th>0.8</th>\n      <th>0.7</th>\n      <th>0.6</th>\n      <th>0.5</th>\n      <th>0.4</th>\n      <th>0.3</th>\n      <th>0.2</th>\n      <th>0.1</th>\n      <th>0.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>claim</th>\n      <td>491010</td>\n      <td>1380</td>\n      <td>453</td>\n      <td>257</td>\n      <td>139</td>\n      <td>93</td>\n      <td>58</td>\n      <td>36</td>\n      <td>29</td>\n      <td>13</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}