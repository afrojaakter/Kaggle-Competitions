{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, time, random, datetime\n",
    "%matplotlib inline\n",
    "#Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import (OneHotEncoder, \n",
    "LabelEncoder, label_binarize)\n",
    "\n",
    "#Machine learning\n",
    "import catboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import (model_selection, tree, preprocessing,\n",
    "metrics, linear_model)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import (LinearRegression, \n",
    "LogisticRegression, SGDClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "#rebels and ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train and test data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "gender_submission = pd.read_csv('gender_submission.csv') #e.g of what a submisision should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View the training data\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Age.plot.hist()\n",
    "#train.Fare.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the test data\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the example submission dataframe\n",
    "gender_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the data statistics\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphic of missing values\n",
    "missingno.matrix(train, figsize = (10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see the number of missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Perform data analysis, we are going to see two new dataframe:\n",
    "    i. to explore discretised variables\n",
    "    ii. to explore continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = pd.DataFrame() #for discretised continuous variables\n",
    "df_con = pd.DataFrame() # for continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the info of the number of people survived\n",
    "fig = plt.figure(figsize = (5,1))\n",
    "sns.countplot(y ='Survived', data = train)\n",
    "print(train.Survived.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding  this to subset dataframes\n",
    "df_bin['Survived'] = train['Survived']\n",
    "df_con['Survived'] = train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.head()\n",
    "print(len(df_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the idea of the data distribution according to the passenger  class\n",
    "sns.distplot(train.Pclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give the number of missing variables in Pclass\n",
    "train.Pclass.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there  is no missing  values adding Pclass in the sub-dataframes\n",
    "df_bin['Pclass'] = train['Pclass']\n",
    "df_con['Pclass'] = train['Pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives the total  number of different names in the data\n",
    "train.Name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives the visualization of Sex distribution\n",
    "plt.figure(figsize = (5,1))\n",
    "sns.countplot(y = 'Sex', data = train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the  missing value in the sex column\n",
    "train.Sex.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding to the sub-dataframe\n",
    "df_bin['Sex'] = train['Sex']\n",
    "df_bin['Sex'] = np.where(df_bin['Sex'] == 'female', 1,0)\n",
    "df_con['Sex'] = train['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#givees the visualization of the Sex variable compared to Survivla\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "sns.distplot(df_bin.loc[df_bin['Survived'] == 1]['Sex'], kde_kws = {'label':'Survived'})\n",
    "sns.distplot(df_bin.loc[df_bin['Survived'] == 0]['Sex'], kde_kws = {'label':'Did not survived'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data in Age\n",
    "train.Age.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create count and distribution visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCountDist(data,bin_df, label_column, target_column, \n",
    "                  figsize, use_bin_df = False):\n",
    "    \"\"\"\n",
    "    Function to plot counts and distributions of a \n",
    "    label variable and target variable side by side.\n",
    "    data = target dataframe\n",
    "    bin_df = binned dataframe for countplot\n",
    "    label_column = binary labelled column\n",
    "    target_column = column we want to view counts and distributions\n",
    "    use_bin_df = whether or not to use the bin_df, default False\n",
    "    \"\"\"\n",
    "    if use_bin_df:\n",
    "        fig = plt.figure(figsize = figsize)\n",
    "        plt.subplot(1,2,1)\n",
    "        sns.countplot(y = target_column, data = bin_df);\n",
    "        plt.subplot(1,2,2)\n",
    "        sns.distplot(data.loc[data[label_column] == 1][target_column],\n",
    "                     kde_kws = {\"label\":\"survived\"})\n",
    "        sns.distplot(data.loc[data[label_column] == 0][target_column],\n",
    "                    kde_kws = {\"label\":\"Did not survived\"})\n",
    "        \n",
    "    else:\n",
    "        fig = plt.figure(figsize = figsize)\n",
    "        plt.subplot(1,2,1)\n",
    "        sns.countplot(y = target_column, data = data);\n",
    "        plt.subplot(1,2,2)\n",
    "        sns.distplot(data.loc[data[label_column] == 1][target_column],\n",
    "                     kde_kws = {\"label\":\"survived\"})\n",
    "        sns.distplot(data.loc[data[label_column] == 0][target_column],\n",
    "                    kde_kws = {\"label\":\"Did not survived\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.SibSp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.SibSp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin['SibSp'] = train['SibSp']\n",
    "df_con['SibSp'] = train['SibSp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the counts of sibling and spouse and the distribution of the values\n",
    "#against survived\n",
    "plotCountDist(train, \n",
    "              bin_df = df_bin,\n",
    "              label_column = 'Survived',\n",
    "              target_column = 'SibSp',\n",
    "              figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Parch.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Parch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add parch to the sub-dataframes\n",
    "df_bin['Parch'] = train['Parch']\n",
    "df_con['Parch'] = train['Parch']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise the counts of Parch and the distribution of the values against survived\n",
    "plotCountDist(train,\n",
    "             bin_df = df_bin,\n",
    "             label_column = 'Survived',\n",
    "             target_column = 'Parch',\n",
    "             figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Ticket.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many types of tickets were there?\n",
    "sns.countplot(y = 'Ticket', data = train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Ticket.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique tickets\n",
    "print(\"There are {} unique Tickets values.\".format\n",
    "      (len(train.Ticket.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Fare.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y='Fare', data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Fare.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.Fare.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Fare to the sub-dataframe\n",
    "df_con['Fare'] = train['Fare']\n",
    "df_bin['Fare'] = pd.cut(train['Fare'], bins = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.Fare.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCountDist(data = train,\n",
    "             bin_df = df_bin,\n",
    "             label_column= 'Survived',\n",
    "             target_column= 'Fare',\n",
    "             figsize= (20,10),\n",
    "             use_bin_df = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Cabin.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Cabin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Embarked.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Embarked.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y = 'Embarked', data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Embarked to subdataframe\n",
    "df_bin['Embarked'] = train['Embarked']\n",
    "df_con['Embarked'] = train['Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing rows from the data\n",
    "print(len(df_con))\n",
    "df_con = df_con.dropna(subset = ['Embarked'])\n",
    "df_bin =df_bin.dropna(subset = ['Embarked'])\n",
    "print(len(df_con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to drop a column from the dataframe in pandas\n",
    "#df_bin.drop(['Name'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#One-hot encode binned variables\n",
    "one_hot_cols = df_bin.columns.tolist()\n",
    "one_hot_cols.remove('Survived')\n",
    "df_bin_enc = pd.get_dummies(df_bin, columns = one_hot_cols)\n",
    "df_bin_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con_enc = df_con.apply(LabelEncoder().fit_transform)\n",
    "df_con_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot enconde the categorical columns\n",
    "df_embarked_one_hot = pd.get_dummies(df_con['Embarked'], prefix = 'embarked')\n",
    "df_sex_one_hot = pd.get_dummies(df_con['Sex'], prefix = 'sex')\n",
    "df_plcass_one_hot = pd.get_dummies(df_con['Pclass'], prefix = 'pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the one hot encoded columns with df_con_enc\n",
    "df_con_enc = pd.concat([df_con,\n",
    "                       df_embarked_one_hot,\n",
    "                       df_sex_one_hot,\n",
    "                       df_plcass_one_hot], axis = 1)\n",
    "#Drop the original categorical columns (because now they've been one hot eoncoded)\n",
    "df_con_enc = df_con_enc.drop(['Pclass','Sex','Embarked'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the dataframe we want to use first for predictions\n",
    "selected_df = df_con_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataframe into data and lables\n",
    "TrainData = selected_df.drop('Survived',axis = 1)\n",
    "TrainLabels = selected_df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TrainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLabels.shape, TrainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit function that runs the requested algorithm and returns the accuracy metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "def fit_ml_algorithm(algorithm, TrainData, TrainLabels, cv):\n",
    "    \"\"\"\n",
    "    algorithm: learning algorithm\n",
    "    TrainData: Train Input Data\n",
    "    TrainLables: Train Output Data\n",
    "    Cross-Validation (CV): The training data is split into k smaller sets\n",
    "                           - The model is trained on (k-1) of the folds as training data;\n",
    "                           - The resulting model is validated on the remianign part of \n",
    "                             the data (i.e., it is used to compute a preformance measure \n",
    "                             such as accuracy).\n",
    "    n_jobs: The number of CPUs to used to do the computation. \n",
    "            -1 means using all the processors.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    model = algorithm.fit(TrainData, TrainLabels)\n",
    "    acc = round(model.score(TrainData, TrainLabels)*100,2)\n",
    "    \n",
    "    #Cross Validation\n",
    "    train_pred = model_selection.cross_val_predict(algorithm, TrainData, \n",
    "                                                 TrainLabels,\n",
    "                                                 cv = cv,\n",
    "                                                 n_jobs = -1)\n",
    "    acc_cv = round(metrics.accuracy_score(TrainLabels, train_pred)*100,2)\n",
    "    log_time = (time.time() - start_time)\n",
    "    \n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Acuracy CV 10-Fold: \", acc_cv)\n",
    "    print(\"Running Time: \", datetime.timedelta(seconds = log_time))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "fit_ml_algorithm(LogisticRegression(),TrainData, TrainLabels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Nearest Neighbours\n",
    "fit_ml_algorithm(KNeighborsClassifier(),TrainData, TrainLabels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Naive Nayes\n",
    "fit_ml_algorithm(GaussianNB(),TrainData, TrainLabels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Support Vector Machines (SVC)\n",
    "fit_ml_algorithm(LinearSVC(),TrainData, TrainLabels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent\n",
    "fit_ml_algorithm(SGDClassifier(),TrainData, TrainLabels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier\n",
    "\n",
    "fit_ml_algorithm(DecisionTreeClassifier(),TrainData, TrainLabels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boost Trees\n",
    "fit_ml_algorithm(GradientBoostingClassifier(),TrainData, TrainLabels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CatBoost ALgorithm\n",
    "fit_ml_algorithm(CatBoostClassifier(),TrainData, TrainLabels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = np.where(TrainData.dtypes != np.float)[0]\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(TrainData, TrainLabels, cat_features)\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=1000,\n",
    "                                    custom_loss=['Accuracy'],\n",
    "                                    loss_function='Logloss')\n",
    "\n",
    "# Fit CatBoost model\n",
    "catboost_model.fit(train_pool,\n",
    "                   plot=True)\n",
    "\n",
    "# CatBoost accuracy\n",
    "acc_catboost = round(catboost_model.score(TrainData, TrainLabels) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long will this take?\n",
    "start_time = time.time()\n",
    "\n",
    "# Set params for cross-validation as same as initial model\n",
    "cv_params = catboost_model.get_params()\n",
    "\n",
    "# Run the cross-validation for 10-folds (same as the other models)\n",
    "cv_data = cv(train_pool,\n",
    "             cv_params,\n",
    "             fold_count=10,\n",
    "             plot=True)\n",
    "\n",
    "# How long did it take?\n",
    "catboost_time = (time.time() - start_time)\n",
    "\n",
    "# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\n",
    "acc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the CatBoost model metrics\n",
    "print(\"---CatBoost Metrics---\")\n",
    "print(\"Accuracy: {}\".format(acc_catboost))\n",
    "print(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))\n",
    "print(\"Running Time: {}\".format(datetime.timedelta(seconds=catboost_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the CatBoost model has the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, data):\n",
    "    fea_imp = pd.DataFrame({'importance': model.feature_importances_, \n",
    "                           'features': data.columns})\n",
    "    fea_imp = fea_imp.sort_values(['importance','features'], ascending=[True, False]).iloc[-10:]\n",
    "    _ = fea_imp.plot(kind = 'barh', x = 'features', y = 'importance', figsize = (20,10))\n",
    "    return fea_imp\n",
    "#plt.savefig('catboost_feature_importance.png')\n",
    "feature_importance(catboost_model, TrainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "Precision: A metric which measures a models ability to correctly identify onely relevant instances\n",
    "Recall: A metric which measures a models ability to find all the relevant cases in a dataset\n",
    "Combination of precision and recall gives F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Precision', 'Recall', 'F1', 'AUC']\n",
    "eval_metrics = catboost_model.eval_metrics(train_pool, metrics = metrics, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    print(str(metric)+\":{}\".format(np.mean(eval_metrics[metric])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low recall means there's a higher amount of false negatives(predicting did not survibed when it actually survived\n",
    "\n",
    "Precision is higher means less false positive(i.e predicting survibed when it actually not survibed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission into kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode the columns of the test data\n",
    "test_embarked_one_hot = pd.get_dummies(test['Embarked'],\n",
    "                                       prefix = 'embarked')\n",
    "\n",
    "test_sex_one_hot = pd.get_dummies(test['Sex'],\n",
    "                                       prefix = 'sex')\n",
    "\n",
    "test_pclass_one_hot = pd.get_dummies(test['Pclass'],\n",
    "                                       prefix = 'pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the test one hot encoded columns with the test data\n",
    "test1 = pd.concat([test,\n",
    "                 test_embarked_one_hot,\n",
    "                  test_sex_one_hot,\n",
    "                 test_pclass_one_hot], axis = 1)\n",
    "test1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test1.iloc[:, 5:]  #gives all rows and columns from a to b\n",
    "test1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test1.drop(['Cabin', 'Embarked'], axis = 1)\n",
    "test1 = test1.drop(['Ticket'], axis = 1)\n",
    "test1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the train and test dataset has same type of data with equal number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.shape, TrainData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test and train data are in same format. Now we can make prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a prediction using the catboost model on the test dataset\n",
    "predictions = catboost_model.predict(test1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a submission dataframe and append the relevant columns\n",
    "submission = pd.DataFrame()\n",
    "submission['PassengerId'] = test['PassengerId']\n",
    "submission['Survived']  = predictions #assigning the model predictions on the test dataset\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The submission data should look like the following\n",
    "gender_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting submission dataframe 'Survived' column into integers\n",
    "submission['Survived'] = submission['Survived'].astype(int)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the test and submission dataframe has same length or no\n",
    "if len(submission) == len(test):\n",
    "    print(\"Submission dataframe has same length as test dataframe: {} rows\".format(len(submission)))\n",
    "else:\n",
    "    print(\"Dataframe mismatched and  not be able to submit to Kaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting submission dataframe into csv format\n",
    "submission.to_csv('../catboost_sibmission.csv', index = False)\n",
    "print(\"submissin CSV is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the submission csv to make it is in right format\n",
    "submission_check = pd.read_csv(\"../catboost_sibmission.csv\")\n",
    "submission_check.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
